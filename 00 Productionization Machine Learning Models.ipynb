{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "when do we have Productizing Model\n",
    "\n",
    "We have the model,Which has been been trained on data offline, now we want run the model in real world.\n",
    "It takes real world input and generates output.\n",
    "\n",
    "Different ways to Productizing ML Model\n",
    "    1. Using Sk Learn\n",
    "    2. Custom Implementtaion Approach\n",
    "    \n",
    "Using Sk Learn\n",
    "    - In Scikit Learn we have something called Model Persistence, here we store the model model into local in Disk(HDD,SDD).\n",
    "      whenever we want run model we load into to RAM from the disk.\n",
    "      \n",
    "    - Pickle and Joblib are the terms you will hear quite often during model persistence.\n",
    "    \n",
    "Custom Implementaion Approach\n",
    "    - Here we store all of the parameters of the model to a file the way i want.\n",
    "      Example if the model is Logistic Regression i store weights.\n",
    "      if the mdoel is random forest i store as if else condition\n",
    "      \n",
    "What is the need to Custom Implementaion\n",
    "    - becasue of low latency. At end of the SK Learn is not very fast\n",
    "    - We can Implement the Cutsom inference using C/C++/java native language because of Fast execution(Speed).\n",
    "    \n",
    "Model is deployed when High Throughput(Lots of query points in short time)?\n",
    "  - Here we have multiple boxes to handle traffic using load balancer\n",
    "  - Each contains CPU, Load balancer gets all the request and send percetage of request to CPU1 and CPU2 and CPU3...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design Approaches for Deployment\n",
    "\n",
    "This is one way, we deploy our model. The model is running on one box, and we push the request with data (the data we named as \n",
    "featurized data, discussed earlier) to the box and it will respond with an outcome (it could be a recommendation of similar \n",
    "products, movies, predicting prices, etc.).\n",
    "\n",
    "For the request, the server; the server that sends requests and ML server that gives output agrees on the format of the \n",
    "data (format is critical to the process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retraining Model Periodically\n",
    "\n",
    "    - Lets say Model 'M10' is built on the last 10 days data (d1,d2....d10), now we have Productionzed model 'M10' and on \n",
    "      day11 we have observed the perfromance metrics(lets say AUC) is around 80% and day12 it is (85%) now coming to day13\n",
    "      it is around 60% model is perfromed bad here, so here there is a signal we need to retrain model with most recent data.\n",
    "      \n",
    "How to Determine if the model is needs to to retrain\n",
    "  - Model performance is dropping\n",
    "  - Dataset itself changing (Check the distrbution of the data) called non stationary data.\n",
    "\n",
    "Best Practices:\n",
    "    - Retrain the Model Regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Challenges while Training Model\n",
    "\n",
    "  - Volume of the data\n",
    "  - Does Model improves if Data size increases\n",
    "  - Lets say performnace of model increases as data size increases but we need to find out where does model \n",
    "    performance not increases.\n",
    "  - If we have small amount of the data (Lets say 30 GB) it is going to fit int the RAM.\n",
    "    here we can use SCIKIT Learn,RStudio, We can use XG Boost\n",
    "    \n",
    "   - If we have dataset of size Medium to Large, here we can use SCIKIT Learn or other but we have to go for Samplinging, which \n",
    "     can again drop the performance.\n",
    "     Here we can define Strategy where we split data into n parts and try Logistic Regression finally go for Aggreation of \n",
    "     output all n models, mostly average.\n",
    "\n",
    "   - Image we want to train model on 10 Million data points(Large Data), where data will not fit into single harddisk.\n",
    "     The best option hers is Spark.\n",
    "     Spark is the distributed computing platform. and spark provides MLLibs options for Machine Learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Different Options Where you can Deploy Machine Leanirng model.\n",
    "\n",
    "  1. Deploying machine learning models as web services\n",
    "      - Using Flask Web Service\n",
    "\n",
    "  2. Deploying machine learning models for batch prediction\n",
    "  \n",
    "  3. Deploying machine learning models on edge devices as embedded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Different approches to Deploy Machine Learning Models on Cloud platform\n",
    "\n",
    "1. AWS EC2\n",
    "    a.Training a machine learning model on a local system.\n",
    "    b.Wrapping the inference logic into a flask application.\n",
    "    c.Using docker to containerize the flask application.\n",
    "    d.Hosting the docker container on an AWS ec2 instance and consuming the web-service\n",
    "    \n",
    "2. Azure Functions\n",
    "    a. Register the model’s .pkl file to Azure Machine Learning\n",
    "    b. Prepare a scoring script\n",
    "    c. Create an inference config\n",
    "    d. Create a Docker image containing your machine learning function\n",
    "    \n",
    "3. Microsoft Azure Container Instance\n",
    "\n",
    "4. Heroku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Undrerstading usage Dockers in Machine Learinng Model Productionization\n",
    "\n",
    "To put machine learning into production, let’s consider that the application needs to be broken down into smaller \n",
    "micro-services such as ingestion, preparation, combination, separation, training, evaluation, inference, postprocessing and \n",
    "monitoring.\n",
    "\n",
    "Dokerize the Machine Model Model, Push the targeted Image into the Azure Container Registry(ACR) and then \n",
    "to Azure Container Instance(ACI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
