{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Pipelines :  \n",
    "\n",
    "Extract data from a data source, apply operations, such as transformations, filters, joins, or aggregations to the data, and \n",
    "publish the processed data to a data sink (target data machine).\n",
    "\n",
    "A data pipeline may be a simple process of data extraction and loading, or, it may be designed to handle data in a more \n",
    "advanced manner, such as training datasets for machine learning.\n",
    "\n",
    "Need for AWS Data Pipeline ?\n",
    "    Data is growing exponentially and that too at a faster pace. Companies of all sizes are realizing that managing, processing,\n",
    "    storing & migrating the data has become more complicated & time-consuming than in the past.\n",
    "    So, listed below are some of the issues that companies are facing with ever increasing data: \n",
    "\n",
    "    Bulk amount of Data: There is a lot of raw & unprocessed data. \n",
    "        There are log files, \n",
    "        Demographic data\n",
    "        Data collected from sensors\n",
    "        Transaction histories & lot more.\n",
    "\n",
    "    Different data stores: \n",
    "        There are a variety of data storage options. \n",
    "        Companies have their own data warehouse, cloud-based storage like Amazon S3, \n",
    "            Amazon Relational Database Service(RDS) & database servers running on EC2 instances. \n",
    "\n",
    "    Time-consuming & costly: Managing bulk of data is time-consuming & a very expensive. A lot of money is to be spent on \n",
    "    transform, store & process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Ingestion\n",
    "    - The first step of the pipeline is data ingestion. This stage will be responsible for running the extractors that will \n",
    "      collect data from the different sources and load them into the data lake.\n",
    "      \n",
    "Data Storage :\n",
    "    - Once the scripts extracted the data from the different data sources, the data was loaded into S3.\n",
    "    - You can create three directories here, like that:\n",
    "        - Raw: here you will store data in its true form, the way it came from the source without modifications.\n",
    "        \n",
    "        - Transformed: after transforming data, treating possible problems such as standardization, missing values and those \n",
    "          kind of problems, data will be loaded here. That data will be useful for data scientists.\n",
    "          \n",
    "        - Data enrichment: Refers to the process of appending or otherwise enhancing collected data with relevant context \n",
    "          obtained from additional sources.For analysis you will have to enrich data\n",
    "          \n",
    "        - Data warehouse : Now that your data is already on your data lake, transformed and enriched, it is time to send it \n",
    "          to a data warehouse.\n",
    "\n",
    "Source: Data sources may include relational databases and data from SaaS applications.\n",
    "Most pipelines ingest raw data from multiple sources via a push mechanism, an API call, \n",
    "\n",
    "a replication engine that pulls data at regular intervals, or a webhook. Also, the data may be synchronized in real time or at \n",
    "scheduled intervals.\n",
    "\n",
    "Destination: A destination may be a data store — such as an on-premises or cloud-based data warehouse, a data lake, or a \n",
    "data mart — or it may be a BI or analytics application.\n",
    "\n",
    "Transformation: Transformation refers to operations that change data, which may include data standardization, sorting, \n",
    "deduplication, validation, and verification. The ultimate goal is to make it possible to analyze the data.\n",
    "\n",
    "Processing: There are two data ingestion models: batch processing, in which source data is collected periodically and sent to \n",
    "the destination system, and stream processing, in which data is sourced, manipulated, and loaded as soon as it’s created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Batch processing \n",
    "    - Refers to processing of high volume of data in batch within a specific time span.\n",
    "    - Batch processing is where the processing happens of blocks of data that have already been stored over a period of time.\n",
    "    - Batch processing is used in payroll and billing system, food processing system etc.\n",
    "    - Hadoop MapReduce is the best framework for processing data in batches.\n",
    "\n",
    "Stream Processing\n",
    "    - Refers to processing of continuous stream of data immediately as it is produced.\n",
    "    - Stream processing allows us to process data in real time as they arrive and quickly detect conditions within small \n",
    "      time period from the \n",
    "      point of receiving the data\n",
    "    - Stream processing is used in stock market, e-commerce transactions, social media etc.\n",
    "    - open source stream processing platforms such as Apache Kafka, Apache Flink, Apache Storm, Apache Samza, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building an ETL Pipeline with Batch Processing\n",
    "\n",
    "You process data in batches from source databases to a data warehouse.\n",
    "\n",
    "To build an ETL pipeline with batch processing, you need to:\n",
    "    - Create reference data: create a dataset that defines the set of permissible values your data may contain. \n",
    "      For example, in a country data field, specify the list of country codes allowed.\n",
    "      \n",
    "    - Extract data from different sources:  the basis for the success of subsequent ETL steps is to extract data correctly.\n",
    "      Take data from a range of sources, such as APIs, non/relational databases, XML, JSON, CSV files, and convert it into a \n",
    "      single format for standardized processing.\n",
    "    \n",
    "    - Validate data: Keep data that have values in the expected ranges and reject any that do not. \n",
    "      For example, if you only want dates from the last year, reject any values older than 12 months.\n",
    "      Analyze rejected records, on an on-going basis, to identify issues, correct the source data, and modify the extraction \n",
    "      process to resolve the problem in future batches\n",
    "      \n",
    "    - Transform data: Remove duplicate data (cleaning), apply business rules, check data integrity (ensure that data has not \n",
    "      been corrupted or lost), and create aggregates as necessary.\n",
    "      For example, if you want to analyze revenue, you can summarize the dollar amount of invoices into a daily or monthly \n",
    "      total. You need to program numerous functions to transform the data automatically.\n",
    "      \n",
    "    - Publish to your data warehouse: Load data to the target tables. \n",
    "      Some data warehouses overwrite existing information whenever the ETL pipeline loads a new batch - this might happen daily,\n",
    "      weekly, or monthly. In other cases, the ETL workflow can add data without overwriting, including a timestamp to indicate \n",
    "      it is new.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
