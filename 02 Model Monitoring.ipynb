{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assumption during Ml Deployment :\n",
    "    When we deploy models to production and expect to observe error rates, like those we saw during model evaluation, we are \n",
    "    making an assumption that future data will be similar to past observed data. \n",
    "    \n",
    "    Specifically, we are assuming that the distributions of the features and targets will remain fairly constant.\n",
    "    But this assumption usually does not hold. Trends change over time, people’s interests vary with the seasons, and the stock \n",
    "    market ebbs and wanes. And so our models must adapt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Drift :\n",
    "    Model Drift refers to a model’s predictive performance degrading over time due to a change in the environment that violates \n",
    "    the model’s assumptions.\n",
    "    \n",
    "Model Monitoring:\n",
    "    - Model monitoring is the close tracking of the performance of ML models in production and teams can identify potential \n",
    "      issues before they impact the business.\n",
    "    - Model Monitoring is an operational stage in the machine learning life cycle that comes after model deployment, and it \n",
    "      entails ‘monitoring’ your ML models for things like errors, crashes, and latency, but most importantly, to ensure that \n",
    "      your model is maintaining a predetermined desired level of performance.\n",
    "    - The mode of monitoring can either be manual, or an automated script that triggers alarms and notifications whenever \n",
    "      sudden anomalies are observed.\n",
    "\n",
    "EXAMINING THE CORRELATIONS BETWEEN FEATURES\n",
    "    - Many models assume that the relationships between features must remain fixed. \n",
    "    - Therefore you’ll also want to monitor pairwise correlations between individual input features\n",
    "    \n",
    "Points to Monitor\n",
    "    - Assess data relevance (the variance between the data models were trained on and the live data it is scoring against \n",
    "      across all features)\n",
    "    - Model performance (model accuracy)\n",
    "    - Trust elements such as fairness and bias, and of course, business impact\n",
    "    \n",
    "There are a number of different things to monitor per feature including:\n",
    "    - The range of possible values\n",
    "    - Histograms of values\n",
    "    - Whether the feature accepts NULLs and if so, the number of NULLs expected\n",
    "    \n",
    "Model Drift:\n",
    "    - Target models will degrade over time as you use them, known as model drift.\n",
    "    - Model Drift, also known as model decay, refers to the degradation of a model’s prediction power due to various reasons\n",
    "    \n",
    "Why Model Drift?\n",
    "    - Unseen data\n",
    "    - Changes in the environment and relationships between variables\n",
    "    - Upstream data changes\n",
    "    \n",
    "Why Model Monitoring is Important?\n",
    "\n",
    "    - Model would have trained with specific set of data.\n",
    "    - A model may go out of context if there is data skew i.e. data distribution may have changed in production from what was \n",
    "      used during training.\n",
    "    - the user behavior may have changed ex: It may also be that a feature becomes unavailable in production data or that the \n",
    "      model may no longer be relevant as the real-world environment might have changed\n",
    "    - Monitoring the changes in model’s behavior and the characteristics of the most recent data used at inference is thus of \n",
    "      utmost importance. \n",
    "    - This ensures that the model remains relevant and/or true to the desired performance as promised during the model training \n",
    "      phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Model Monitoring Metrics \n",
    "    1. Stability Metrics\n",
    "        - These metrics help us to capture two types of data distribution shifts\n",
    "        - Metrics\n",
    "            - Prior Probability Shift \n",
    "            - Covariate Shift \n",
    "            - \n",
    "    2. Performance Metrics\n",
    "        - These metrics help us to detect a concept shift in data i.e. identify whether the relation between independent & \n",
    "          dependent variables has changed (e.g. post-COVID the way users purchase during festivals may have changed).\n",
    "        - They do so by examining how good or bad the existing deployed model is performing viz-a-viz when it was trained \n",
    "          (scenario I) or during a previous time frame post-deployment (scenario II). \n",
    "        - Metrics\n",
    "            - Project Metrics like RMSE, R-Square, etc for regression and accuracy, AUC-ROC, etc for classification.\n",
    "            - Gini and KS -Statistics: A statistical measure of how well the predicted probabilities/classes are separated \n",
    "              (only for classification models)\n",
    "\n",
    "    3. Operation Metrics\n",
    "        - These metrics help us to determine how the deployed model is performing from a usage point of view.\n",
    "        - Metrics\n",
    "            - # of time ML API endpoints called in the past\n",
    "            - Latency when calling ML API endpoints\n",
    "            - IO/Memory/CPU usage when performing prediction\n",
    "            - System uptim\n",
    "            - Disk utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There are many reasons why a model can fail to make useful predictions in production:\n",
    "    - The underlying data distribution has shifted over time and the model has gone stale.\n",
    "    - The production data stream contains edge cases (not seen during model development) where the model performs poorly.\n",
    "    - The model was misconfigured in its production deployment.\n",
    "\n",
    "What should we be monitoring?\n",
    "    Model metrics\n",
    "        - Prediction distributions\n",
    "        - Feature distributions\n",
    "        - Evaluation metrics (when ground truth is available)\n",
    "\n",
    "    System metrics\n",
    "        - Request throughput\n",
    "        - Error rate\n",
    "        - Request latencies\n",
    "        - Request body size\n",
    "        - Response body size\n",
    "\n",
    "    Resource metrics\n",
    "        - CPU utilization\n",
    "        - Memory utilization\n",
    "        - Network data transfer\n",
    "        - Disk I/O\n",
    "\n",
    "Model drift can be classified into two broad categories.\n",
    "    - Concept drift\n",
    "    - Data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Data Drift: Data drift occurs when production data diverges from the model’s original training data. \n",
    "Data drift can happen for a variety of reasons, including a changing business environment, evolving user behavior and interest,\n",
    "modifications to data from third-party sources, data quality issues, and even issues in upstream data processing pipelines.\n",
    "\n",
    "Concept Drift: Concept drift occurs when the expectations of what constitutes a correct prediction change over time even though \n",
    "the distribution of the input data has not changed.\n",
    "For example, loan applicants who were considered as attractive prospects last year (when the training dataset was created) may \n",
    "no longer be considered attractive because of changes in a bank’s strategy or outlook on future macroeconomic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Causes of data drift :\n",
    "    - Upstream process changes, such as a sensor being replaced that changes the units of measurement from inches to centimeters\n",
    "    - Data quality issues, such as a broken sensor always reading 0.\n",
    "    - Natural drift in the data, such as mean temperature changing with the seasons.\n",
    "    - Change in relation between features, or covariate shift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Once drift is detected, you drill down into which features are causing the drift. \n",
    "You then inspect feature level metrics to debug and isolate the root cause for the drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Detect these drifts?\n",
    "    - Since both drifts involve a statistical change in the data, the best approach to detect them is by monitoring its \n",
    "      statistical properties, the model’s predictions, and their correlation with other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Action Items needs to be performed when Model Drift happens?\n",
    "    - The product owner or the quality assurance expert needs to be alerted.\n",
    "    - The Model needs to be switched or Updated\n",
    "    - Re-training the pipelines should be triggred to re-train and update the model as per the latest data or needs\n",
    "\n",
    "\n",
    "Govern : Monitoring and nalyzing is done to govern the deployed application to drive optimal performance for the business.\n",
    "    After Moitoring and analyzing the prediction data, we can generate certain alert and action to govern the system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
